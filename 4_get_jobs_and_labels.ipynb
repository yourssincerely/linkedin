{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf210a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.linkedin import *\n",
    "from functions.labeling import *\n",
    "from functions.db import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6385ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_classification(job_alerts:list):\n",
    "    \n",
    "    #model\n",
    "    classifier = Bert_for_zero_shot_classification()\n",
    "    #tresholds\n",
    "    with open(\"tresholds.json\", \"r\") as f:\n",
    "        tresholds = json.load(f)\n",
    "        treshold_python = tresholds[\"python\"]\n",
    "        treshold_analyst = tresholds[\"analytics\"]\n",
    "        treshold_sql = tresholds[\"sql\"]\n",
    "        treshold_junior = tresholds[\"junior\"]\n",
    "        treshold_overall = tresholds[\"overall\"]\n",
    "    #ids already analysed\n",
    "    duplicates = pd.concat([sql_query_to_pandas(\"SELECT job_id FROM jobs_good_match\"), \n",
    "                    sql_query_to_pandas(\"SELECT job_id FROM jobs_not_good_match\")])\n",
    "    \n",
    "    #info array\n",
    "    good_match = []\n",
    "    not_good_match = []\n",
    "    \n",
    "    #job alerts loop\n",
    "    for job_alert in job_alerts:\n",
    "        driver = linkedin_job_alerts()\n",
    "        content = driver.find_elements(By.TAG_NAME, \"body\")[0]\n",
    "        get_job_alert_page(content, job_alert)\n",
    "        content = driver.find_elements(By.TAG_NAME, \"body\")[0]\n",
    "        \n",
    "        #results page loop setup\n",
    "        next_page = 2\n",
    "        total_pages = get_job_results_page_buttons_total_number(content)\n",
    "        next_page_button = get_next_page_job_button(content, next_page)      \n",
    "        while next_page <= total_pages:\n",
    "            \n",
    "            #jobs on page loop\n",
    "            job_posts_container = job_posts_on_page(content)\n",
    "            job_count = 1\n",
    "            skipped = 0\n",
    "            for job_element in job_posts_container:\n",
    "                print(f\"Getting {job_alert} jobs. Page {next_page - 1} of {total_pages}. Job {job_count} of {len(job_posts_container)}...                       \", \n",
    "                    end = \"\\r\", flush = True)\n",
    "                try:\n",
    "                    job_element.click()\n",
    "                    time.sleep(random.randint(14, 23) / 10)\n",
    "                    content = driver.find_elements(By.TAG_NAME, \"body\")[0]\n",
    "                    job_id, job_description, job_url = get_job_info(content)\n",
    "        \n",
    "                    #avoiding duplication\n",
    "                    if job_id in duplicates.values:\n",
    "                        job_count += 1\n",
    "                        skipped += 1\n",
    "                        pass\n",
    "                    else:\n",
    "                        score_python = pipe(classifier, \"python\", job_description)\n",
    "                        score_analyst = pipe(classifier, \"data analysis\" , job_description)\n",
    "                        score_sql = pipe(classifier, \"sql\", job_description)\n",
    "                        score_junior = pipe(classifier, \"junior level\", job_description)\n",
    "                        score_overall = 3*score_python + 1.5*score_analyst + score_sql\n",
    "                        row = [job_id, job_description, job_url, score_python, score_analyst, score_sql, score_junior, score_overall]\n",
    "\n",
    "                        #classifying the data\n",
    "                        if score_python >= treshold_python and score_junior >= treshold_junior:\n",
    "                            good_match.append(row)\n",
    "                        elif score_overall >= treshold_overall and score_junior >= treshold_junior:\n",
    "                            good_match.append(row)\n",
    "                        elif score_analyst >= treshold_analyst and score_junior >= treshold_junior:\n",
    "                            good_match.append(row)\n",
    "                        elif score_sql >= treshold_sql and score_junior >= treshold_junior:\n",
    "                            good_match.append(row)\n",
    "                        else:\n",
    "                            not_good_match.append(row)\n",
    "                        job_count += 1  \n",
    "                except:\n",
    "                    job_count += 1\n",
    "                    pass\n",
    "            #avoiding unnecessary loops\n",
    "            if skipped == len(job_posts_container):\n",
    "                print(\"\\n\", \"We already have all these jobs... Exiting...\", sep = \"\")\n",
    "                break\n",
    "                \n",
    "            #results page loop end        \n",
    "            try:\n",
    "                next_page_button.click()\n",
    "                next_page += 1\n",
    "                time.sleep(random.randint(20, 27) / 10)\n",
    "                content = driver.find_elements(By.TAG_NAME, \"body\")[0]\n",
    "                next_page_button = get_next_page_job_button(content, next_page)\n",
    "            except:\n",
    "                next_page += 1\n",
    "                \n",
    "    #storing data            \n",
    "    colnames = [\"job_id\", \"job_description\", \"job_url\", \"score_python\", \"score_analyst\", \"score_sql\", \"score_junior\", \"score_overall\"]   \n",
    "    print(\"\\n\", \"\\n\", sep = \"\")\n",
    "\n",
    "    if len(good_match) != 0:\n",
    "        good_match_df = pd.DataFrame(good_match, columns = colnames)\n",
    "        good_match_df.drop_duplicates(subset = [\"job_id\"], keep = 'first', inplace = True)\n",
    "        pandas_to_mysql(good_match_df, \"jobs_good_match\", if_exists = \"append\")\n",
    "\n",
    "    if len(not_good_match) != 0:\n",
    "        not_good_match_df = pd.DataFrame(not_good_match, columns = colnames)\n",
    "        not_good_match_df.drop_duplicates(subset = [\"job_id\"], keep = 'first', inplace = True)\n",
    "        pandas_to_mysql(not_good_match_df, \"jobs_not_good_match\", if_exists = \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42fb25ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data scientist jobs. Page 1 of 40. Job 7 of 24...                       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruben\\anaconda3\\envs\\linkedin\\lib\\site-packages\\transformers\\pipelines\\base.py:1073: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data scientist jobs. Page 22 of 40. Job 25 of 25...                       \n",
      "We already have all these jobs... Exiting...\n",
      "Getting data analyst jobs. Page 32 of 40. Job 25 of 25...                       \n",
      "We already have all these jobs... Exiting...\n",
      "Getting data engineer jobs. Page 39 of 40. Job 25 of 25...                       \n",
      "\n",
      "\n",
      "52 entries were added to the table jobs_good_match!\n",
      "1206 entries were added to the table jobs_not_good_match!\n"
     ]
    }
   ],
   "source": [
    "job_classification([\"data scientist\", \"data analyst\", \"data engineer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
